好的，这是对 AI 短视频生成器项目的总结，按照您要求的要点进行组织：

**1. 项目需求**

* **目标:** 构建一个 Web 应用，允许用户通过输入文本提示词 (prompt)，自动生成一个包含多个场景的短视频。每个场景应包含由 AI 生成的图像和相应的文本转语音 (TTS) 配音，形式类似于“有声图文电子书”。
* **用户故事:**
    * 作为一名内容创作者/教育者/家长，我想要输入一个主题（如儿童故事、古诗词、知识点），以便系统能自动生成一个包含相关图像和配音的短视频，用于快速创建吸引人的教学或娱乐内容。
    * 作为一名普通用户，我想要输入一个想法或故事梗概，以便系统能将其可视化并配音成短视频，方便观看和分享。
* **功能性需求:**
    * 提供 Web 界面接收用户输入的文本提示词。
    * 后端能够调用 OpenAI GPT API 生成文本内容（故事、脚本等）。
    * 后端能够将生成的文本分割成逻辑场景。
    * 后端能够为每个场景调用 OpenAI DALL-E API 生成相关图像。
    * 后端能够为每个场景的文本调用 OpenAI TTS API 生成配音。
    * 后端能够使用视频处理库（如 Moviepy）将各场景的图像和音频按顺序合成为一个视频文件（如 .mp4）。
    * Web 界面应显示生成过程的（基本）状态指示。
    * 生成完成后，Web 界面应能直接播放生成的视频。
* **非功能性需求:**
    * **易用性:** 界面简洁直观，用户只需输入提示词即可启动生成。
    * **安全性:** OpenAI API Key 需安全存储，不能暴露在前端或代码库中。
    * **响应性:** （基本要求）系统应在合理时间内完成视频生成，并向用户反馈状态。生产环境可能需要异步处理。
    * **可访问性:** 应用应能通过标准 Web 浏览器访问。
* **验收标准:**
    * 用户在网页输入提示词并点击生成按钮后，界面显示“生成中”状态。
    * 后端成功调用 OpenAI 的 GPT、DALL-E、TTS 服务，没有出现 API 错误。
    * 在服务器的 `static/output` 目录下成功生成一个 `.mp4` 格式的视频文件。
    * 生成成功后，网页上的视频播放器加载并能播放该视频。
    * 播放的视频包含多个不同的图像场景，且每个场景的配音内容与原始生成文本对应。

**2. 应用流程及流程图**

* **用户与系统交互流程:**
    1.  **访问:** 用户通过浏览器访问应用主页 (`/`)。
    2.  **输入:** 用户在文本框中输入提示词。
    3.  **触发:** 用户点击“生成视频”按钮。
    4.  **请求:** 前端 JavaScript 发送包含提示词的 POST 请求到后端 API (`/generate`)。
    5.  **反馈 (前端):** 界面显示“生成中”状态，按钮暂时禁用。
    6.  **处理 (后端):**
        * 接收请求，验证提示词。
        * 调用 GPT 生成文本内容。
        * 将文本分割成场景。
        * **循环处理每个场景：**
            * 调用 DALL-E 生成图像，保存图像文件。
            * 调用 TTS 生成音频，保存音频文件。
        * 使用 Moviepy 合成各场景的图像和音频片段。
        * 拼接所有片段成最终视频文件，保存。
    7.  **响应 (后端):** 返回包含视频文件 URL 的 JSON 数据（成功时）或错误信息的 JSON 数据（失败时）。
    8.  **反馈 (前端):**
        * **成功:** 接收到视频 URL，更新 `<video>` 元素的 `src` 属性，显示视频播放器，隐藏加载状态，恢复按钮。
        * **失败:** 接收到错误信息，在状态区域显示错误，隐藏加载状态，恢复按钮。
    9.  **播放:** 用户可以在页面上点击播放按钮观看视频。

* **流程图:**

    ```mermaid
    graph TD
        A[Start: User Visits Page] --> B{User Enters Prompt?};
        B -- Yes --> C[User Clicks Generate Button];
        C --> D[Frontend: Show Loading / Disable Button];
        D --> E[Frontend: Send POST /generate Request with Prompt];
        E --> F[Backend: Receive Request];
        F --> G{Generate Text (GPT) OK?};
        G -- Yes --> H[Backend: Split Text into Scenes];
        H --> I[Backend: Loop Through Scenes];
        I --> J{Generate Image (DALL-E) OK?};
        J -- Yes --> K{Generate Audio (TTS) OK?};
        K -- Yes --> L[Backend: Store Image/Audio Files];
        L --> M{More Scenes?};
        M -- Yes --> I;
        M -- No --> N[Backend: Combine Images & Audio into Video (Moviepy)];
        N --> O{Video Creation OK?};
        O -- Yes --> P[Backend: Save Final Video File];
        P --> Q[Backend: Construct Video URL];
        Q --> R[Backend: Send Success JSON (Video URL)];
        R --> S[Frontend: Receive Response];
        S --> T{Response OK?};
        T -- Yes --> U[Frontend: Show Video Player, Load URL, Hide Loading];
        U --> V[End: User Can Play Video];
        T -- No --> W[Frontend: Show Error Message, Hide Loading];
        W --> X[End: Error Displayed];

        %% Error Paths
        G -- No --> Z[Backend: Send Error JSON];
        J -- No --> Z;
        K -- No --> Z;
        O -- No --> Z;
        Z --> S;

        %% Style
        style A fill:#f9f,stroke:#333,stroke-width:2px;
        style V fill:#ccf,stroke:#333,stroke-width:2px;
        style X fill:#fcc,stroke:#333,stroke-width:2px;
    ```

**3. 技术栈概览**

* **后端语言:** **Python (>=3.8)**
    * *为什么:* 生态成熟，拥有优秀的 Web 框架、数据处理和 AI/ML 库，与 OpenAI API 交互方便。
* **后端框架:** **Flask (>=2.0)**
    * *为什么:* 轻量级、灵活、易于上手，适合构建中小型 Web 应用和 API 服务。本项目功能单一，Flask 足够满足需求。
* **AI 服务:** **OpenAI API**
    * *为什么:* 提供高质量的文本生成 (GPT-3.5/GPT-4)、图像生成 (DALL-E 2/3) 和文本转语音 (TTS-1) 功能，是项目核心能力的来源。
    * *SDK:* `openai` Python 库 (>=1.0.0)。
* **视频处理:** **Moviepy (>=1.0.3)**
    * *为什么:* Python 库，能相对简单地进行视频编辑操作（如合并、添加音频、调整大小），避免了直接操作 `ffmpeg` 的复杂性。
    * *依赖:* 需要系统安装 `ffmpeg`。
* **前端技术:** **HTML5, CSS3, JavaScript (ES6+)**
    * *为什么:* Web 开发的基石，实现用户界面、交互逻辑和与后端 API 的通信（使用 `Workspace` API）。
* **环境管理:** **python-dotenv**
    * *为什么:* 用于从 `.env` 文件加载环境变量（如 API Key），实现敏感信息的安全管理。
* **依赖管理:** **pip** 和 `requirements.txt` 文件。
* **服务器 (开发):** Flask 内建开发服务器。
* **服务器 (生产):** (推荐) Gunicorn/uWSGI + Nginx。

**4. 前端与后端指南**

* **后端 (Flask/Python):**
    * *文件夹结构:* 如前所示 (`app.py`, `static/`, `templates/`, `.env`, `requirements.txt`)。复杂功能可拆分到 `utils/` 或使用 Flask Blueprints。
    * *编码规范:* 遵循 PEP 8。使用类型提示 (Type Hinting) 提高代码可读性和健壮性。添加 Docstrings 解释函数/类。
    * *架构模式:* 简单的单体应用。API 端点 (`/generate`) 采用 RESTful 风格（虽然只有一个主要端点）。目前是同步处理，生产环境建议改为异步任务队列（如 Celery）。
    * *API 协议:* 使用 JSON 进行请求和响应。
        * 请求 (`POST /generate`): `{ "prompt": "用户输入的提示词" }`
        * 成功响应 (200 OK): `{ "video_url": "/static/output/generated_video.mp4" }`
        * 失败响应 (400 Bad Request / 500 Internal Server Error): `{ "error": "错误描述信息" }`
    * *测试策略:* (建议) 使用 `pytest` 进行单元测试（测试辅助函数、API 逻辑）和集成测试（测试 API 端点）。
    * *风格规则:* 使用 Linter (如 Flake8) 和 Formatter (如 Black) 保持代码风格一致。
* **前端 (HTML/CSS/JS):**
    * *文件夹结构:* `static/css/style.css`, `static/js/script.js`, `templates/index.html`。
    * *编码规范:* 使用语义化 HTML 标签。CSS 采用 BEM 或其他模块化方法。JavaScript 使用 ES6+ 语法，如 `async/await`, `Workspace`, `const/let`。代码应有适当注释。
    * *架构模式:* 简单的客户端渲染逻辑。通过 `script.js` 操作 DOM，响应用户事件，并通过 `Workspace` API 与后端交互。
    * *API 交互:* 使用 `Workspace` API 发送异步请求，处理 Promise。根据后端响应更新 UI 状态（加载、成功、错误）。
    * *测试策略:* (建议) 手动测试关键用户流程。对于复杂前端，可引入单元测试 (Jest) 或端到端测试 (Cypress/Playwright)。
    * *风格规则:* 使用 Linter (如 ESLint) 和 Formatter (如 Prettier) 保持代码风格一致。

**5. API、SDK 与技术参考**

* **OpenAI API:**
    * *SDK:* `openai` (Python library), version >= 1.0.0
    * *文档:* [https://platform.openai.com/docs/](https://platform.openai.com/docs/)
    * *服务 & 端点 (via SDK):*
        * `client.chat.completions.create()` (Model: `gpt-3.5-turbo` / `gpt-4`) - 用于文本生成。
        * `client.images.generate()` (Model: `dall-e-2` / `dall-e-3`) - 用于图像生成。
        * `client.audio.speech.create()` (Model: `tts-1`) - 用于语音合成。
    * *配置:* 需要设置 `OPENAI_API_KEY` 环境变量。
    * *示例 (Python):*
        ```python
        from openai import OpenAI
        client = OpenAI() # Assumes OPENAI_API_KEY is set in env

        # Text Generation
        chat_completion = client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Tell a short story."}])
        story = chat_completion.choices[0].message.content

        # Image Generation
        image_response = client.images.generate(model="dall-e-2", prompt="A cute robot waving", size="1024x1024", n=1)
        image_url = image_response.data[0].url

        # Speech Synthesis
        speech_response = client.audio.speech.create(model="tts-1", voice="alloy", input="Hello world!")
        speech_response.stream_to_file("output.mp3")
        ```
* **Moviepy:**
    * *库:* `moviepy` (Python library), version >= 1.0.3
    * *文档:* [https://zulko.github.io/moviepy/](https://zulko.github.io/moviepy/)
    * *依赖:* `ffmpeg` (必须安装在系统路径中)
    * *常用类/函数:* `ImageClip`, `AudioFileClip`, `concatenate_videoclips`, `set_duration`, `set_audio`, `write_videofile`, `resize`, `CompositeVideoClip`.
    * *示例 (Python):*
        ```python
        from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips

        audio = AudioFileClip("scene1_audio.mp3")
        image = ImageClip("scene1_image.png").set_duration(audio.duration)
        clip1 = image.set_audio(audio)

        # ... create clip2, clip3 ...

        final_video = concatenate_videoclips([clip1, clip2])
        final_video.write_videofile("final_output.mp4", fps=24, codec='libx264', audio_codec='aac')
        clip1.close() # Important to close clips
        final_video.close()
        ```
* **Flask:**
    * *库:* `Flask` (Python library), version >= 2.0
    * *文档:* [https://flask.palletsprojects.com/](https://flask.palletsprojects.com/)
    * *常用模块/函数:* `Flask`, `request`, `jsonify`, `render_template`, `url_for`, `@app.route`.
* **Fetch API:**
    * *标准:* Web API (内置于浏览器)
    * *文档:* [https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)
    * *用途:* 在 JavaScript 中发起异步 HTTP 请求。
    * *示例 (JavaScript):*
        ```javascript
        async function generateVideo(prompt) {
          try {
            const response = await fetch('/generate', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ prompt: prompt })
            });
            if (!response.ok) {
              const errorData = await response.json();
              throw new Error(errorData.error || 'Failed to generate');
            }
            const data = await response.json();
            console.log("Video URL:", data.video_url);
            // Update UI here
          } catch (error) {
            console.error("Error:", error);
            // Update UI with error message
          }
        }
        ```