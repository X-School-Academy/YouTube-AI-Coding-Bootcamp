by Gemini 2.5 Pro

好的，这是一个使用 Python (Flask 作为后端框架)、HTML/CSS/JavaScript (前端) 和 OpenAI API (文本、语音、图像) 来实现自动生成“有声图文电子书”式短视频的项目方案。

**核心思路:**

1.  **前端 (HTML/CSS/JS):** 提供用户界面，接收用户输入的提示词 (prompt)，发送请求到后端，显示生成进度，并在完成后播放视频。
2.  **后端 (Python/Flask):**
    * 接收前端的请求和提示词。
    * 调用 OpenAI GPT API 根据提示词生成故事或内容文本。
    * 将文本分割成多个场景 (例如，按段落或句子)。
    * 对于每个场景：
        * 调用 OpenAI DALL-E API 根据场景内容生成对应的图像。
        * 调用 OpenAI TTS API 将场景文本转换成语音。
    * 使用视频处理库 (如 `moviepy`) 将每个场景的图像和语音合成为一个视频片段。
    * 将所有场景的视频片段拼接成最终的完整视频。
    * 将生成的视频文件路径返回给前端。
3.  **OpenAI API:** 提供核心的生成能力 (文本、图像、语音)。

---

**项目结构和文件:**

```
auto-video-generator/
│
├── app.py                   # Flask 后端应用主文件
├── requirements.txt         # Python 依赖库列表
├── .env                     # 存储 OpenAI API Key (重要！不要上传到版本库)
├── static/                  # 存放静态文件 (CSS, JS, 生成的视频)
│   ├── css/
│   │   └── style.css        # CSS 样式文件
│   ├── js/
│   │   └── script.js        # JavaScript 文件
│   └── output/              # 存放生成的视频、图片、音频 (需要写入权限)
│       └── .gitkeep         # 保持目录存在
└── templates/               # 存放 HTML 模板
    └── index.html           # 前端主页面
```

---

**文件内容:**

**1. `.env` (环境变量文件)**

```plaintext
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # 替换为你的 OpenAI API Key
```
* **重要:** 确保将此文件添加到你的 `.gitignore` 文件中，避免泄露 API Key。

**2. `requirements.txt` (Python 依赖)**

```text
Flask
openai
python-dotenv
moviepy
requests # openai 库可能需要
```
* 安装依赖: `pip install -r requirements.txt`
* **注意:** `moviepy` 可能需要系统安装 `ffmpeg`。如果遇到问题，请根据 `moviepy` 文档安装 `ffmpeg`。对于文本叠加，可能还需要 `ImageMagick`。

**3. `app.py` (Flask 后端)**

```python
import os
import uuid
import re
from flask import Flask, request, jsonify, render_template, url_for
from openai import OpenAI
from moviepy.editor import *
from dotenv import load_dotenv
import requests # 用于下载图片

load_dotenv() # 加载 .env 文件中的环境变量

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'static/output' # 设置上传/生成文件目录

# --- OpenAI API 配置 ---
try:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if not client.api_key:
        raise ValueError("OpenAI API key not found in .env file")
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    # 在实际应用中，可能需要更健壮的错误处理或退出
    client = None # 设置为 None 以便后续检查

# --- Helper 函数 ---
def generate_story(prompt):
    """使用 GPT 生成故事或内容"""
    if not client: return None, "OpenAI client not initialized."
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # 或者使用 gpt-4
            messages=[
                {"role": "system", "content": "You are a creative storyteller. Generate a short story or content based on the user's prompt. Divide the story into distinct paragraphs or scenes, each suitable for a single image and voiceover segment in a short video. Maximum 5 scenes."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        story = response.choices[0].message.content.strip()
        # 按段落分割场景 (跳过空行)
        scenes = [s.strip() for s in story.split('\n') if s.strip()]
        # 限制场景数量
        return scenes[:5], None # 返回最多5个场景
    except Exception as e:
        print(f"Error generating story: {e}")
        return None, f"Failed to generate story: {e}"

def generate_image(prompt, image_path):
    """使用 DALL-E 生成图像并保存"""
    if not client: return False, "OpenAI client not initialized."
    try:
        # 为 DALL-E 创建更具体的图像提示
        image_prompt = f"Illustration style, simple and clear, depicting: {prompt}"
        response = client.images.generate(
            model="dall-e-2", # 或者 dall-e-3
            prompt=image_prompt,
            size="1024x1024", # DALL-E 2 支持的尺寸
            quality="standard",
            n=1,
        )
        image_url = response.data[0].url
        # 下载图片
        img_data = requests.get(image_url).content
        with open(image_path, 'wb') as handler:
            handler.write(img_data)
        return True, None
    except Exception as e:
        print(f"Error generating image: {e}")
        return False, f"Failed to generate image: {e}"

def generate_audio(text, audio_path):
    """使用 TTS 生成语音并保存"""
    if not client: return False, "OpenAI client not initialized."
    try:
        response = client.audio.speech.create(
            model="tts-1", # 或者 tts-1-hd
            voice="alloy", # 可选: alloy, echo, fable, onyx, nova, shimmer
            input=text
        )
        response.stream_to_file(audio_path)
        return True, None
    except Exception as e:
        print(f"Error generating audio: {e}")
        return False, f"Failed to generate audio: {e}"

def create_video_segment(image_path, audio_path, output_path):
    """创建单个场景的视频片段"""
    try:
        audio_clip = AudioFileClip(audio_path)
        # 确保图像大小适合视频 (例如 1080x1920 或 1920x1080)
        # 这里我们先用原始 DALL-E 尺寸，可以根据需要调整
        image_clip = ImageClip(image_path).set_duration(audio_clip.duration)
        # 设置视频尺寸 (例如 9:16 竖屏) - DALL-E 2 是方形，需要裁剪或加背景
        # 为了简单起见，我们先用方形，可以在后面 composite 时调整
        # image_clip = image_clip.resize(height=1920) # 调整高度
        # image_clip = image_clip.crop(x_center=image_clip.w/2, y_center=image_clip.h/2, width=1080, height=1920) # 裁剪

        video_clip = image_clip.set_audio(audio_clip)
        video_clip.write_videofile(output_path, codec='libx264', audio_codec='aac', fps=24) # 确保设置编解码器
        audio_clip.close() # 关闭文件句柄
        image_clip.close()
        video_clip.close()
        return True, None
    except Exception as e:
        print(f"Error creating video segment: {e}")
        # 尝试关闭可能打开的剪辑
        try: audio_clip.close()
        except: pass
        try: image_clip.close()
        except: pass
        try: video_clip.close()
        except: pass
        return False, f"Failed to create video segment: {e}"


def combine_videos(video_paths, final_output_path):
    """合并多个视频片段"""
    try:
        clips = [VideoFileClip(path) for path in video_paths]
        final_clip = concatenate_videoclips(clips, method="compose")
        # 可以设定最终视频的尺寸在这里，如果需要统一
        # final_clip = final_clip.resize(width=1080) # 例如，统一宽度
        final_clip.write_videofile(final_output_path, codec='libx264', audio_codec='aac', fps=24)
        # 关闭所有文件句柄
        for clip in clips:
            clip.close()
        final_clip.close()
        return True, None
    except Exception as e:
        print(f"Error combining videos: {e}")
        # 尝试关闭可能打开的剪辑
        try:
            for clip in clips: clip.close()
        except: pass
        try: final_clip.close()
        except: pass
        return False, f"Failed to combine videos: {e}"


# --- Flask 路由 ---
@app.route('/')
def index():
    """渲染主页"""
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate_video_route():
    """处理视频生成请求"""
    if not client:
         return jsonify({"error": "OpenAI client is not configured. Check API key."}), 500

    prompt = request.json.get('prompt')
    if not prompt:
        return jsonify({"error": "Prompt is required"}), 400

    print(f"Received prompt: {prompt}")

    # --- 1. 生成故事文本 ---
    scenes, error = generate_story(prompt)
    if error:
        return jsonify({"error": error}), 500
    if not scenes:
         return jsonify({"error": "Generated empty story."}), 500
    print(f"Generated {len(scenes)} scenes.")

    base_filename = str(uuid.uuid4())
    segment_paths = []
    cleanup_files = [] # 记录所有中间文件以便清理

    try:
        # --- 2. 为每个场景生成图像和音频，并创建视频片段 ---
        for i, scene_text in enumerate(scenes):
            print(f"Processing scene {i+1}/{len(scenes)}: {scene_text[:50]}...")
            scene_suffix = f"{base_filename}_scene_{i+1}"
            image_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{scene_suffix}.png")
            audio_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{scene_suffix}.mp3")
            segment_video_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{scene_suffix}.mp4")

            cleanup_files.extend([image_path, audio_path, segment_video_path])

            # 生成图像
            print(f"  Generating image for scene {i+1}...")
            success, error = generate_image(scene_text, image_path)
            if not success:
                raise Exception(f"Scene {i+1} image generation failed: {error}")
            print(f"  Image saved to {image_path}")

            # 生成音频
            print(f"  Generating audio for scene {i+1}...")
            success, error = generate_audio(scene_text, audio_path)
            if not success:
                 raise Exception(f"Scene {i+1} audio generation failed: {error}")
            print(f"  Audio saved to {audio_path}")

            # 创建视频片段 (暂时不用) - 直接在最后合并时处理
            # print(f"  Creating video segment for scene {i+1}...")
            # success, error = create_video_segment(image_path, audio_path, segment_video_path)
            # if not success:
            #      raise Exception(f"Scene {i+1} video segment creation failed: {error}")
            # print(f"  Video segment saved to {segment_video_path}")
            # segment_paths.append(segment_video_path)

        # --- 3. 使用 Moviepy 合并 ---
        print("Combining video segments...")
        final_video_filename = f"{base_filename}_final.mp4"
        final_video_path_abs = os.path.join(app.config['UPLOAD_FOLDER'], final_video_filename)
        cleanup_files.append(final_video_path_abs) # 添加最终文件到清理列表

        video_clips = []
        audio_clips = []
        try:
            for i in range(len(scenes)):
                scene_suffix = f"{base_filename}_scene_{i+1}"
                image_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{scene_suffix}.png")
                audio_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{scene_suffix}.mp3")

                audio_clip = AudioFileClip(audio_path)
                audio_clips.append(audio_clip) # 保留引用以便关闭

                # 创建图像剪辑，设置持续时间为音频长度
                # 调整图像大小以适应常见的视频格式，例如 1080p (1920x1080) 或竖屏 (1080x1920)
                # 这里我们创建一个 1080x1080 的方形视频 (DALL-E 2 输出方形)
                img_clip = ImageClip(image_path).set_duration(audio_clip.duration).resize(height=1080).set_position('center')
                # 如果需要背景填充到 1920x1080
                # bg = ColorClip(size=(1920, 1080), color=(0,0,0), duration=audio_clip.duration)
                # img_clip_on_bg = CompositeVideoClip([bg, img_clip])
                # video_clips.append(img_clip_on_bg.set_audio(audio_clip))

                # 为了简单，我们直接用方形并设置音频
                video_clips.append(img_clip.set_audio(audio_clip))

            # 拼接所有场景
            final_clip = concatenate_videoclips(video_clips, method="compose")
            # 设置输出参数
            final_clip.write_videofile(final_video_path_abs, codec='libx264', audio_codec='aac', fps=24, threads=4, logger='bar') # 使用多线程加速
            final_clip.close() # 关闭最终剪辑
            # 关闭所有中间剪辑
            for clip in video_clips: clip.close()
            for clip in audio_clips: clip.close()

        except Exception as e_moviepy:
            # 确保关闭所有可能打开的剪辑文件
            try: final_clip.close()
            except: pass
            for clip in video_clips:
                try: clip.close()
                except: pass
            for clip in audio_clips:
                try: clip.close()
                except: pass
            raise Exception(f"Moviepy video creation failed: {e_moviepy}")


        # --- 4. 返回视频路径 ---
        # 使用 url_for 生成相对 URL，这样即使部署在子路径下也能工作
        video_url = url_for('static', filename=f'output/{final_video_filename}')
        print(f"Final video ready: {video_url}")

        return jsonify({"video_url": video_url})

    except Exception as e:
        print(f"An error occurred during video generation: {e}")
        return jsonify({"error": f"An error occurred: {str(e)}"}), 500

    finally:
        # --- 5. 清理临时文件 (可选) ---
        # 注意：最终的视频文件也在这里被删除了，如果需要保留最终视频，
        # 需要将 final_video_path_abs 从 cleanup_files 中移除
        # 暂时注释掉清理，方便调试
        # print("Cleaning up temporary files...")
        # for f_path in cleanup_files:
        #     if os.path.exists(f_path):
        #         try:
        #             os.remove(f_path)
        #             print(f"  Removed: {f_path}")
        #         except Exception as clean_e:
        #             print(f"  Error removing {f_path}: {clean_e}")
        pass # 暂时不清理

if __name__ == '__main__':
    # 确保 output 目录存在
    if not os.path.exists(app.config['UPLOAD_FOLDER']):
        os.makedirs(app.config['UPLOAD_FOLDER'])
    # 运行 Flask 应用
    # debug=True 会在代码更改时自动重载，但生产环境应设为 False
    # host='0.0.0.0' 允许从局域网访问
    app.run(debug=True, host='0.0.0.0', port=5000)

```

**4. `templates/index.html` (前端页面)**

```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 短视频生成器</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <div class="container">
        <h1>AI 短视频生成器</h1>
        <p>输入一个主题或故事梗概，AI 将自动生成图文并茂的有声视频。</p>

        <div class="input-area">
            <textarea id="prompt-input" rows="4" placeholder="例如：一个关于勇敢小松鼠寻找丢失橡果的儿童故事"></textarea>
            <button id="generate-button">生成视频</button>
        </div>

        <div id="status-area">
            <p id="status-message"></p>
            <div class="loader" id="loader" style="display: none;"></div>
        </div>

        <div id="video-area" style="display: none;">
            <h2>生成的视频:</h2>
            <video id="video-player" width="80%" controls>
                您的浏览器不支持 Video 标签。
            </video>
        </div>
    </div>

    <script src="{{ url_for('static', filename='js/script.js') }}"></script>
</body>
</html>
```

**5. `static/css/style.css` (样式)**

```css
body {
    font-family: sans-serif;
    line-height: 1.6;
    margin: 0;
    padding: 20px;
    background-color: #f4f4f4;
    color: #333;
}

.container {
    max-width: 800px;
    margin: 20px auto;
    padding: 30px;
    background-color: #fff;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    text-align: center; /* Center align container content */
}

h1 {
    color: #444;
    margin-bottom: 15px;
}

p {
    margin-bottom: 20px;
    color: #555;
}

.input-area {
    margin-bottom: 30px;
    display: flex; /* Use flexbox for alignment */
    flex-direction: column; /* Stack items vertically */
    align-items: center; /* Center items horizontally */
}

textarea#prompt-input {
    width: 90%; /* Take most of the width */
    padding: 12px;
    margin-bottom: 15px; /* Space below textarea */
    border: 1px solid #ccc;
    border-radius: 4px;
    font-size: 1rem;
    resize: vertical; /* Allow vertical resize */
}

button#generate-button {
    padding: 12px 25px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-size: 1rem;
    transition: background-color 0.3s ease;
}

button#generate-button:hover {
    background-color: #0056b3;
}

button#generate-button:disabled {
    background-color: #cccccc;
    cursor: not-allowed;
}


#status-area {
    margin-top: 20px;
    min-height: 50px; /* Ensure space even when empty */
    text-align: center;
}

#status-message {
    font-weight: bold;
    margin-bottom: 10px; /* Space below message */
}

/* Simple CSS Loader */
.loader {
    border: 5px solid #f3f3f3; /* Light grey */
    border-top: 5px solid #3498db; /* Blue */
    border-radius: 50%;
    width: 40px;
    height: 40px;
    animation: spin 1s linear infinite;
    margin: 10px auto; /* Centered */
    display: none; /* Hidden by default */
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}


#video-area {
    margin-top: 30px;
    text-align: center; /* Center video player */
}

video#video-player {
    max-width: 100%; /* Ensure video is responsive */
    border: 1px solid #ddd;
    border-radius: 4px;
    background-color: #000; /* Black background for letterboxing */
}
```

**6. `static/js/script.js` (前端逻辑)**

```javascript
document.addEventListener('DOMContentLoaded', () => {
    const promptInput = document.getElementById('prompt-input');
    const generateButton = document.getElementById('generate-button');
    const statusMessage = document.getElementById('status-message');
    const loader = document.getElementById('loader');
    const videoArea = document.getElementById('video-area');
    const videoPlayer = document.getElementById('video-player');

    generateButton.addEventListener('click', async () => {
        const prompt = promptInput.value.trim();
        if (!prompt) {
            statusMessage.textContent = '请输入提示词！';
            statusMessage.style.color = 'red';
            return;
        }

        // --- UI Update: Start Generation ---
        statusMessage.textContent = '正在生成视频，请稍候... 这可能需要几分钟时间。';
        statusMessage.style.color = 'black';
        loader.style.display = 'block'; // Show loader
        videoArea.style.display = 'none'; // Hide previous video
        generateButton.disabled = true; // Disable button
        promptInput.disabled = true; // Disable input

        try {
            // --- Send request to backend ---
            const response = await fetch('/generate', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ prompt: prompt }),
            });

            // --- Process Response ---
            const result = await response.json();

            if (response.ok) {
                statusMessage.textContent = '视频生成成功！';
                statusMessage.style.color = 'green';
                videoPlayer.src = result.video_url; // Set video source
                videoArea.style.display = 'block'; // Show video area
                videoPlayer.load(); // Load the new video
                // videoPlayer.play(); // Optional: Auto-play
            } else {
                // Display error from backend JSON response
                statusMessage.textContent = `生成失败: ${result.error || '未知错误'}`;
                statusMessage.style.color = 'red';
            }
        } catch (error) {
            // Handle network errors or other exceptions
            console.error('Generation request failed:', error);
            statusMessage.textContent = `发生错误: ${error.message}`;
            statusMessage.style.color = 'red';
        } finally {
            // --- UI Update: End Generation (Success or Failure) ---
            loader.style.display = 'none'; // Hide loader
            generateButton.disabled = false; // Re-enable button
            promptInput.disabled = false; // Re-enable input
        }
    });
});
```

---

**如何运行:**

1.  **安装依赖:** 在项目根目录 (`auto-video-generator/`) 打开终端，运行 `pip install -r requirements.txt`。
2.  **安装 ffmpeg:** 如果你的系统没有安装 `ffmpeg`，请根据你的操作系统进行安装。`moviepy` 需要它来处理视频。
3.  **配置 API Key:** 创建 `.env` 文件，并将你的 OpenAI API Key 填入其中 (`OPENAI_API_KEY=sk-yourkey`)。
4.  **运行后端:** 在终端中运行 `python app.py`。Flask 服务器将在 `http://127.0.0.1:5000/` (或者 `http://0.0.0.0:5000/`) 启动。
5.  **访问前端:** 在浏览器中打开 Flask 服务器显示的地址 (通常是 `http://127.0.0.1:5000/`)。
6.  **使用:** 在文本框中输入你的提示词，点击 "生成视频" 按钮，等待处理完成。视频生成后会显示在页面下方。

---

**注意事项和可扩展点:**

1.  **API 成本:** OpenAI API 调用是收费的 (文本、图像、语音)。生成一个视频会调用多次 API，请注意成本。
2.  **性能:** 视频生成（特别是 `moviepy` 合成）是 CPU 密集型任务，可能需要较长时间。对于生产环境，应考虑使用异步任务队列 (如 Celery + Redis/RabbitMQ) 来处理生成请求，避免阻塞 Web 服务器。
3.  **错误处理:** 当前的错误处理比较基础。可以添加更详细的日志记录和用户反馈。
4.  **进度反馈:** 目前仅显示 "正在生成..."。可以通过 WebSocket 或轮询后端状态接口的方式实现更精细的进度条 (例如，"正在生成文本...", "正在生成第 2 张图片...", "正在合成视频...")。
5.  **内容分割:** `generate_story` 中按换行符分割场景可能不够健壮。可以要求 GPT 输出带有明确场景标记 (如 `[SCENE 1]`, `[SCENE 2]`) 的文本，然后据此分割。
6.  **图像提示:** `generate_image` 中的图像提示比较简单。可以尝试让 GPT 为每个场景生成更具体的图像描述，以提高 DALL-E 生成图像的相关性。
7.  **视频样式:** 可以添加更多自定义选项，如选择 TTS 声音、图像风格、视频分辨率、添加背景音乐、文字叠加效果等。`moviepy` 支持这些功能，但会增加代码复杂度。
8.  **资源清理:** `app.py` 中暂时注释掉了临时文件的清理。在生产环境中，应确保生成过程中产生的图片、音频和中间视频片段被及时清理，以防磁盘空间耗尽。最终视频文件是否清理取决于你的需求。
9.  **安全性:** 确保 API Key 存储安全，不要硬编码在代码中或提交到版本控制系统。
10. **依赖:** 确保运行环境满足 `moviepy` (及 `ffmpeg`, 可能还有 `ImageMagick` for text) 的依赖。